{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songmac/2023-Sesac-Lecture-and-Project/blob/master/05_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_RNN%2C_LSTM%2C_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8HULiDrujNi"
      },
      "source": [
        "# <딥러닝-RNN, LSTM, GRU>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 딥러닝-RNN\n",
        "\n",
        "### 1-1. RNN(순환신경망, Recurrent Neural Network)\n",
        "- 가장 기본적인 인공신경망 시퀀스(시계열) 모델\n",
        "- 입력과 출력을 시퀀스 단위로 처리하는 모델\n",
        "- 사건/문맥의 앞에 주어진 정보의 흐름을 활용하여 정답을 예측할 수 있는 모델\n",
        "\n",
        "- DNN구조와 다르게 Ht(히든벡터)의 시간 흐름 및 계산 반영된 히든레이어가 있음\n",
        "- 순차데이터(sequential data) : ‘순서’를 가진 데이터로 순서가 변경될 경우 데이터의 특성을 잃어버리는 데이터를 의미 ex. 문장, 주가, 날씨, DNA 유전정보 등의 시계열 데이터(time series data)\n",
        "\n",
        "\n",
        "### 1-2. RNN layer의 구조적 특징\n",
        "- RNN Layer = 순환층 = cell\n",
        "- RNN Layer의 출력 결과 = hidden state = hidden vector = h\n",
        "- hidden state를 계산 : 사용되는 함수(f)는 tanh(tangent hyperbolic)이며, 입출력(x,y)에 대한 각 Whh, Wxh, Wyh 가중치를 이용하여 Ht와 Yt에 대한 값 계산\n",
        "\n",
        "- (특징1) hidden state 계산을 통해 정보의 지속성 유지\n",
        "- (특징2) 장기의존성 문제(Gradient vanishing/Gradient Exploding) : 예측 문장의 길이가 길어지는 경우 hidden state를 통해 넘겨받게 되는 정보의 비중이 점차 줄어들어 처음 입력된 단어의 반영도는 점차 줄어듦\n",
        "\n",
        "\n",
        "### 1-3. RNN종류\n",
        "-  입출력 개수에 따라 4가지 응용 모형 존재\n",
        "\n",
        "#### 1-3-1. one to many \n",
        "- 하나의 입력 -> RNN 층 통과 -> 여러개의 출력값 내보냄. 출력갯수는 정해져 있지 않고 사용자가 결정 ex. 이미지 캡셔닝(출력값은 토큰으로 자연어처리 필요)\n",
        "#### 1-3-2. many to one\n",
        "- ex. 감정분석(긍정/부정 확률 값으로 판단)\n",
        "#### 1-3-3. many to many\n",
        "- 입력과 출력의 갯수가 같은경우(ex. 개체명 인식-단어의 유형 즉, 사람, 장소, 조직, 시간 등)\n",
        "-‘Seq2Seq’가 요즘 대세(ex. attention model, attention all you need model(Transformer), bert, chatGPT)\n",
        "- 같지않은 경우(ex. 기계번역) 두가지 모형이 존재"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. 딥러닝-LSTM\n",
        "### 2-1. LSTM(장단기 메모리, Long-Short Term Memory)\n",
        "\n",
        "- Simple RNN의 장기의존성문제와 Gradient 문제를 해결하기 위해 고안된 모형\n",
        "- ‘Gate’를 추가하여 기억할것과 잊을것을 선택하여 중요한 정보만 기억하도록 함 (forget/input/output gate 총 3개의 gate 존재, cell state 파악 필요)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2-2. LSTM 구조\n",
        "\n",
        "- 기존 RNN에는 단기상태를 의미하는 Ht(Hidden State)만 존재했는데, LSTM에는 장기상태를 의미하는 Ct(Cell State)가 추가됨.\n",
        "- hidden state(전체 or 마지막), last state, 'last cell state'를 조사해야 함\n",
        "- LSTM은 장기상태(Long-term state)를 의미하는 Ct 벡터가 추가됨\n",
        "- 틸다C: 현재 Cell(순환층)의 정보. 함수 G로 표현하기도 함\n",
        "- Cell State : Forget Gate(Ct벡터, 장기상태) + Input Gate(틸다C, 현재상태) 를 출력\n",
        "- 이전 정보반영에 필요한 계산은 sigmoid, 현재 정보를 다음 셀에 얼마나 반영할 것인가는 tanh를 사용\n",
        "\n",
        "#### 2-2-1. Forget Gate\n",
        "- {입력값으로 H(t-1) 와 Xt를 받아 계산한 후} 시그모이드를 통해 0 ~ 1 사이 값으로 변환하여 전단계에서 받은 정보를 얼마나 잊어버릴지에 대한 값을 결정\n",
        "#### 2-2-2. Input Gate \n",
        "- {입력값으로는 H(t-1) 와 Xt를 받아 계산하고} 하이퍼블릭 탄젠트를 취해 현재 Cell의 정보를 얼마나 반영할지를 계산\n",
        "#### 2-2-3. Output Gate\n",
        "- {입력값으로는 H(t-1) 와 Xt를 받아} 다음 hidden state(H(t+1))로 얼마만큼의 hidden vector(Ht)가 넘어가게 될지 output 게이트를 생성한 후 시그모이드 함수를 통해 0과 1사이의 값으로 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. 딥러닝-GRU\n",
        "### 3-1. GRU(게이트 순환 유닛, Gated Recurrent Unit)\n",
        "\n",
        "- LSTM간소화(gate 개수 : 3 -> 2, 파라미터 수 적음)\n",
        "- Update Gate(유사Forget Gate+Input Gate)로 만들고, Reset Gate(유사Ouput Gate)를 추가함\n",
        "- LSTM과 달리 whole_sequence_output, final_state를 조사해야 함\n",
        "\n",
        "#### 3-1-1. Update Gate\n",
        "- 데이터를 얼마나 활성화 할 것인가 혹은 얼마나 업데이트 할 것인가에 대한 값을 계산. forget gate의 부분과 input gate의 부분을 모두 제어함\n",
        "- forget gate와 유사한 성질\n",
        "- Zt = 1이면 forget gate가 열리고 input gate는 닫힘\n",
        "- 반면, Zt = 0이면 forget gate 닫히고 input gate 열림\n",
        "#### 3-1-2. Reset Gate\n",
        "- 전 단계에서 받은 ht-1와 새로운 입력인 Xt의 각각을 얼마만큼 합칠지에 대한 값을 계산. 즉, 이전 정보에서 얼마만큼을 선택하여 내보낼지에 대한 계산\n",
        "- output gate와 유사한 성질\n",
        "#### 3-1-3. Hidden state \n",
        "- Reset gate 값을 사용하여 현재 셀의 값(틸타C)을 계산하고, 구한 현재 셀의 값과 update gate를 사용하여 새로운 hidden state값을 계산"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNvPCwqP6FOJFsoFbb7UV+L",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
