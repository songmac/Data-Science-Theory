{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songmac/2023-Sesac-Lecture-and-Project/blob/master/05_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWeU0WhNBXsL"
      },
      "source": [
        "# <딥러닝-DNN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zWRRXmarAuO"
      },
      "source": [
        "\n",
        "- 딥러닝 < 머신러닝(알고리즘) < 인공지능\n",
        "- 인공지능 : 머신러닝 기술이 탑재된 실제 형태\n",
        "- 머신러닝 : 데이터를 통해 규칙을 파악\n",
        "- 딥러닝 : 레이블/클래스 와 attribute/feature 로 이루어져 있음\n",
        "- 텐서플로우 : 구글에서 만든 딥러닝 사용 가능한 모듈\n",
        "- 케라스 : 텐서플로우 안의 패키지(모델, 레이어, 함수 활용 가능)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXMrNBXDZxC"
      },
      "source": [
        "## 1. 딥러닝 개요\n",
        "\n",
        "- 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법을 사용\n",
        "- 원-핫 인코딩(One-Hot Encoding)은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpKZok1krYr6"
      },
      "source": [
        "\n",
        "## 2. 딥러닝 구조의 이해\n",
        "- add : 모델 쌓기 (크기, 활성함수, 로스 함수, 최적화 함수 등 설정 필)\n",
        "- compile : 모델을 만들고 난 후 묶어서 실행\n",
        "- fit : 딥러닝 모델 본격적으로 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWWhotJ_C2HX"
      },
      "source": [
        "### 2-1. 딥러닝 구현 순서\n",
        "\n",
        "1) 내가 사용할 패키지, 라이브러리 호출\n",
        "2) 내가 사용할 데이터 정리\n",
        "3) 사용하는 모델 선언\n",
        "4) 심층망 모델 구현 ex. RNN, BERT, CNN, Transformer 등\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQOcXm2tC2aq"
      },
      "source": [
        "## 3. 손실함수, 옵티마이저, 활성화 함수\n",
        "\n",
        "- 데이터가 연속형\n",
        "- 선형 회귀모델 : 오차 값이 가장 작은 직선을 구하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8egds0DisGzV"
      },
      "source": [
        "### 3-1. 선형회귀 모델에 자주 사용되는 손실함수\n",
        "\n",
        "1. MSE(평균 제곱 오차)  : 실제 값과 예측 값 사이의 오차. 제곱 값으로 이상치의 영향을 많이 받음\n",
        "2. RMSE(평균 제곱근 오차, root mean squared error) : 이상치에 대한 민감도가 MSE보다 적음\n",
        "3. MAE(평균 절대값 오차, mean absolute error) : 상대적으로 많이 쓰이지는 않음\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZZqhPmsLzd"
      },
      "source": [
        "### 3-2. 분류모델에 사용되는 손실함수\n",
        "#### 3-2-1. binay crossentropy : 이진분류 모델 학습시 사용\n",
        "- y 값이 0, 1 형태로 제공될 때 사용\n",
        "- 활성함수 : sigmoid 주로 사용\n",
        "#### 3-2-2. categorical crossentropy : 카테고리 3개이상인 경우 사용\n",
        "- y 값이 one-hot encoding 형태로 제공될 때 사용\n",
        "- 활성함수 : softmax 주로 사용\n",
        "#### 3-2-3. sparse categorical crossentropy : 카테고리가 3개 이상인 경우 사용\n",
        "- y 값이 one-hot encoding이 아닌 정수인 상태로 주어졌을때 주로 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzOPt41bsPvi"
      },
      "source": [
        "### 3-3. 옵티마이저\n",
        "\n",
        "\n",
        " 기울기의 최적값을 찾아가는 함수\n",
        "-  학습률 : 기울기의 부호를 바꾸어가며 이동하는 거리. 최적의 학습률을 찾는 과정도 중요(작게 설정하는 게 중요)\n",
        "\n",
        "### 3-3-1. 경사하강법(손실함수이자 옵티마이저, GD) \n",
        "오차의 변화를 2차함수로 만들고, 적절한 학습률(+,- 기울기 바꿔가며) 설정하여 미분값이 0인 지점을 구하는 방법\n",
        "### 3-3-2. 확률적 경사하강법(SGD)\n",
        "랜덤한 일부 데이터만 이용하기 때문에 경사하강법에 비해 현저히 빠름\n",
        "### 3-3-3. 모멘텀(진동) 확률적 경사하강법\n",
        "확률적 경사하강법보다 좀 더 띄엄띄엄 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyhcfu7isSis"
      },
      "source": [
        "\n",
        "### 3-4. 활성화 함수\n",
        "\n",
        "  -  수학 계산식 참고\n",
        "  - 신경망에서 나온 값을 다음 값으로 옮겨주는 역할. Dense 뒤에 위치함\n",
        "  - vanishing gradient 란? 시그모이드와 하이퍼볼릭 탄젠트함수에서는 학습 결과가 underfitting 되는 것 (overfitting의 반대).미분값(기울기)가 점차 0이 됨. 그러나 양수 x에 대해 기울기가 1인 ReLU함수와 그를 보완한 LeakyReLU 함수로 이를 개선\n",
        "  - dropout : 중간중간 overfitting을 막기위해 일부 값을 랜덤으로 drop 시킴\n",
        "\n",
        "#### 3-4-1. 시그모이드 함수 : 밑이 자연상수 e인 함수\n",
        "  - 일정 값 기준으로 x값이 0이상의 각각 1, 0의 값에 가까운 것으로 분류. 따라서 바이너리 분류와 함께 쓰임\n",
        "  - vanishing gradient 문제 존재\n",
        "\n",
        "#### 3-4-2. Tanh 함수 : x값이 0을 기준으로 양수, 음수로 나뉨\n",
        "  - vanishing gradient 문제 존재\n",
        "\n",
        "#### 3-4-3. Softmax 함수\n",
        "  - N가지 클래스 중 하나로 분류하는 Multi-class Classification 에 주로 사용\n",
        "  - 출력 값이 0~1사이 값을 가짐\n",
        "  - 입력 값을 지수함수로 받아 정규화 하여 총합을 1로 만듬\n",
        "  - 주로 출력함수로 많이 사용\n",
        "\n",
        "#### 3-4-4. ReLU 함수\n",
        "  - y=x 선형 함수에서 입력 값 0 이하에서부터 정류(rectified)된 함수\n",
        "  - 구현 간단, 연산속도 빠름\n",
        "  - vanishing gradient 문제 해소\n",
        "\n",
        "#### 3-4-5. Leaky ReLU 함수\n",
        "  - 입력 값이 음수일 때 출력 값을 0이 아닌 0.001과 같이 매우 작은 값으로 출력\n",
        "  - ReLU 보다 더 나은 vanishing gradient 문제 해소 가능\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFBZ2JzdC2j1"
      },
      "source": [
        "\n",
        "## 4. 인공신경망\n",
        "### 4-1.  퍼셉트론\n",
        "\n",
        "\n",
        "- 사람 머리의 뉴런을 구현한 것이 퍼셉트론 모델\n",
        "- 경사하강법이 추가된 모형이 아달라인 모델도 있음\n",
        "- 학습을 하기위해서는 역전파를 수행해야 함\n",
        "- 퍼셉트론 : 입력 값을 여러 개 받아 출력을 만드는데 가중치를 조절할 수 있게 만들어진 구조\n",
        "- XOR 문제(AND, NAND, OR, XOR) : 4개의 게이트 중 XOR만 선을 그을 수 없음 -> 2차원을 공간구조로 바꾸어 (NAND&OR) 퍼셉트론으로 출력 가능\n",
        "\n",
        "### 4-2. 다층 퍼셉트론\n",
        "- 은닉층에 들어있는 가중치 데이터 업데이트를 통해 학습하는 오차 역전파를 이용함\n",
        "- 가중치 데이터의 2차 업데이트부터 값은 알 수 없음\n",
        "- 다중 퍼셉트론 가중치 계산식에서 델타식의 개수에 따라 은닉층 깊이를 알 수 있음\n",
        "- 밑바닥부터 시작하는 딥러닝 책 참고\n",
        "- 오차 역전파(Back propagation) : 각 가중치 매개변수에 대한 값을 미세하게 조정하는 방법\n",
        "- 연쇄 법칙(chain rule) : 함수의 기울기를 구하기 위해 연쇄법칙(합성함수와 편미분 계산)을 이용하여 미분을 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO51EVqxYHTBhu9qYOmeMP8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
