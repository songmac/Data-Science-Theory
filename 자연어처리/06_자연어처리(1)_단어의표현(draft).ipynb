{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songmac/2023-Data-Analysis-Study-Personal/blob/master/06_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC(1)_%EB%8B%A8%EC%96%B4%EC%9D%98%ED%91%9C%ED%98%84(draft).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lghlPMLUwsqD"
      },
      "source": [
        "# <단어의 표현(Word Representation)>\n",
        "- 문자열을 숫자화 하기 위함 (Encoding)\n",
        "- 학습목표 : one-hot vector, N-gram, BoW(Bag of Words), TDM, TF-IDF\n",
        "- 다음주 학습목표 : TF-IDF 예제 풀이, LSA\n",
        "\n",
        "## 1. Local Repesentation\n",
        "- 표현하고자 하는 단어만을 보고 수치화하여 표현\n",
        "### 1-1. one-hot encoding \n",
        "- 표현하고자하는 단어의 갯수를 벡터차원에서 1, 0을 부여하는 방식\n",
        "- 한계 : 차원 문제. 벡터가 단어의 의미를 담지 못함\n",
        "### 1-2. Ngram\n",
        "- n개의 연속된 단어 나열(bi, tri, ...). 문맥 유추 가능\n",
        "### 1-3. Bag of Words\n",
        "- 문서의 단어 출현 순서를 무시한 채, 출현 빈도수만으로 단어를 표현\n",
        "- 생성방법 : 각 토큰에 고유 인덱스 부여 -> 각 인덱스 위치에 토큰 등장 횟수를 기록\n",
        "- 한계 : 같은 의미의 다른 단어 표현이 있을 경우 다른 것으로 인식/인덱싱 될 수 있음. 표현되지 않는 단어가 많아 빈공간(sparse)이 많음. 또한 단어 순서를 고려하지 않고, 빈도수가 중요도를 의미하지는 않음\n",
        "### 1-4. DTM(Document  to Term), TDM(Term to Document)\n",
        "- 문서에 등장하는 각 단어의 등장빈도를 행렬로 표현. 이 또한 표현되지 않는 단어가 많아 빈공간(sparse)이 많음. 또한 단어 순서를 고려하지 않고, 빈도수가 중요도를 의미하지는 않음\n",
        "### 1-5. TF-IDF\n",
        "- 문서집합에서 단어와 문서의 관련성을 평가하는 방법(상대적 중요도 계산)\n",
        "- TDM보다 정확함\n",
        "- '단어빈도 X 역문서빈도' 로 계산\n",
        "- TF 값이 크고, IDF 값도 큰 경우 중요 키워드라 할 수 있음\n",
        "- 계산 절차 : 토큰 Index생성 -> TF 계산 -> IDF 계산 -> TF-IDF 계산\n",
        "- TF(Term Frequency) : 한 문서 내에서 단어가 얼마나 많이 등장 했는지를 의미. '특정단어등장빈도/문서내전체등장단어빈도'로 계산\n",
        "- IDF(Inverse Document Frequency) : 단어가 얼마나 많은 문서에 등장했는지를 나타내는 DF의 역수. 'log(총문서수/단어가등장한문서수)'로 계산. 로그로 계산하는 이유는 큰 값에 다가갈수록 단어 빈도수가 중요도에 끼치는 영향력은 미미하기 때문.\n",
        "\n",
        "\n",
        "## 2. Distributed Representation\n",
        "- 주변 단어를 함께 참고하여 수치화"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOIFisFqu3x5u+xi3gxmzPf",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
