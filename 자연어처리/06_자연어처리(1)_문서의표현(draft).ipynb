{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuK05al1u0Fl/tiD/3faqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songmac/2023-Sesac-Lecture-and-Project/blob/master/06_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88_%EB%AC%B8%EC%84%9C%EC%9D%98%ED%91%9C%ED%98%84(draft).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<자연어처리 기초(1)-단어/문서의 표현>\n",
        "//Numpy, pandas를 이용해야 하는 이유\n",
        "//머신러닝 계산시, 이미지 또는 문자열의 숫자화(벡터화) 필요\n",
        "\n",
        "\n",
        "<<텍스트 전처리>>\n",
        "// 텍스트 전처리란? 텍스트 내 정보를 유지하고 분석의 효율성을 높이기 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
        "1) 토큰화(Tokenization) : 텍스트를 문장(Sentence)/단어(Word)단위로 분리하는 것\n",
        "- 영문은 공백, 한글은 품사를 고려한 형태소 분석으로 분리하면 유의미한 토큰화가 가능\n",
        "2) 형태소(뜻을 가진 가장 작은 단위) 분석\n",
        "3) 품사 태킹/부착(Pos Tagging) : 분리된 각 토큰에 품사(ex. 조사, 접속사 등) 정보를 추가. 필요한 품사를 필터링 하기 위해 사용\n",
        "4) 개체명 인식(NER, Named Entity Recognition) : 사람, 조직, 지역, 날짜 숫자 등 개체 유형을 식별. 검색 엔진 색인에 활용\n",
        "5) 원형 복원 : 각 토큰의 원형 복원을 함으로써 토큰을 표준화. 단어의 수를 줄여 불필요한 데이터 중복 방지\n",
        "- 어간 추출(Stemming) : 품사를 무시하고 규칠에 기반하여 어간을 추출(ex. believes->believ, conversation->convers, organization->organ)\n",
        "- 표제어 추출(Lemmatization) : 품사정보를 유지하여 표제어 추출(believes->belief, conversation->conversation, organization->organization)\n",
        "6) 불용어 처리(Stopwords) : 불필요한/방해되는 토큰이나 품사를 제거\n",
        "\n",
        "<<단어의 표현>>\n",
        "//Local representation(Discrete representation) : 해당 단어 그 자체만 보고 수치화하여 표현\n",
        "//Distributed representation(Continuous representation) : 단어를 표현하기 위해 주변을 참고하여 수치화\n",
        "//TF-IDF가 가장 중요하고 많이 쓰임\n",
        "1) 원-핫 인코딩(One-Hot_Encoding) : 단어(word)를 숫자로 표현하고자 할 때 적용할 수 있는 방법\n",
        "- 표현하고자하는 단어의 인덱스에는 '1', 다른 인덱스에는 '0'을 부여하여 표현하는 벡터표현 방식\n",
        "- 차원 크기의 문제 존재(연산낭비)\n",
        "- 의미와 문맥을 담지 못함(유사도 계산 :cos 계산시 0이 나옴) ex. 원숭이-바나나, 사과-바나나\n",
        "2) 단어 임베딩(Word-Embedding) : 원-핫 인코딩의 한계를 밀집벡터(Dense Vector)로 표현하여 해결. 임의의 위치에 벡터를 생성한 후 같은 문맥이 등장하는 단어를 가까이 표현\n",
        "3) n-Gram : 복수(n)개의 단어로 묶어서 표현하고 문맥을 제한적으로 표현 ex. uni(1)/bi(2)/tri(3) gram\n",
        "- n은 2~4 권장(count, unique이슈 있음)\n",
        "4) BoW(Bag of Words) : 단어주머니 즉, 문서 내 단어 출현 순서는 무시하고 빈도수를 기반으로 문서를 표현하는 방법\n",
        "- 생성 방법 : 각 토큰에 고유 인덱스 부여 -> 각 인덱스 위치에 토큰 등장 횟수를 기록\n",
        "- 단어 빈도수가 중요도를 의미하지는 않음\n",
        "- 벡터공간 낭비로 연산 비효율성 초래(sparse, 공간에 0이 많은 것)하는 한계가 있음\n",
        "- 정제되지 어휘가 있는 매체에서는 활용하기 어려움 ex.소셜\n",
        "-  sklearn, gensim 패키지 통해 구현할 수 있음\n",
        "5-1) TDM(Term-Document Matrix) : BoW중 하나로, 문서에 등장하는 단어 빈도를 행렬로 표현\n",
        "5-2) DTM(Document Term Matrix) : TDM과 행, 렬이 바뀌어 표현된 것\n",
        "5-3) TF-IDF(단어빈도-역문서빈도, Term Frequency-Inverse Document Frequency)\n",
        "- 역문서 빈도 : 단어가 얼마나 많은 문서에 등장했는지를 나타내는 DF의 역수\n",
        "- 단어가 여러 문서에 많이 등장 할수록 IDF는 작아짐\n",
        "- TDM 내 각 단어의 중요성을 가중치로 표현하여 TDM을 사용하는 것보다 더 정확하게 문서비교가 가능\n",
        "- 계산절차 : 토큰 Index 생성 -> TF 계산(ex. 문서1내 토큰등장 빈도/문서1내 토큰등장 빈도) -> IDF 계산(log(총문서수(N)/단어가 등장한 문서 수(nt))) -> TF-IDF 계산\n",
        "- 결과 : 문서1내 '토큰등장 빈도'가 많고, '토큰등장 문서 수' 가 적을 수록 중심 키워드일 가능성 높음\n",
        "\n",
        "\n",
        "\n",
        "<선형대수 기초>\n",
        "<<벡터>>\n",
        "1) 벡터의 연산 : 각 요소에 덧셈, 뺄셈 그리고 스칼라(상수) 곱셈과 나눗셈 가능\n",
        "2) 벡터의 norm(||x||) : 벡터의 크기/길이 계산할 때 활용(각요소의 제곱의 제곱근)\n",
        "3) 벡터의 내적(dot product) : 두 벡터의 개별 분의 곱의 합. x1-y1, x2-y2 등 요소끼리 곱(내적)한 결과가 0이면 서로 직교함(cos유사도 0)\n",
        "\n",
        "<<행렬(Matrix)>>\n",
        "// 행렬은 2차원 숫자 배열\n",
        "// 자연어처리에서는 벡터의 묶음으로써 행렬을 사용(ex. 1 row : 뉴스 하루 데이터)\n",
        "// 행렬 곱은 첫째 행렬의 '행'요소와 둘째 행렬의 '열'요소를 곱함\n",
        "- 대각(선)행렬(diagonal matrix) : 주대각선 상에 위치한 원소가 아닌 나머지가 0인 행렬\n",
        "- 직교행렬(Orthogonal matrix)\n",
        "- 단위행렬 : 행, 열 크기가 같고, 비대각 요소의 값이 모두 0, 대각 요소만 1의 값을 가진 행렬\n",
        "- 전치행렬 : 행, 열이 서로 바뀐 행렬\n",
        "- 역행렬 : X행렬과 곱했을 때 단위행렬 이 되는 X-1 행렬\n",
        "\n",
        "\n",
        "<<공분산행렬(Covariance Matrix)>>\n",
        "- feature간 분산 정도에 대한 정보 보유. 각 feature의 변동이 얼마나 유사한가 X의 공분산 행렬 계산하여 그래프에 표현함 -> 데이터가 선형 변환(shearing)됨\n",
        "- PCA(Principal Component Analysis) : 정보를 많이 보유하고 있는 순으로 주성분 축을 찾음"
      ],
      "metadata": {
        "id": "yTLqTJkF3_hv"
      }
    }
  ]
}