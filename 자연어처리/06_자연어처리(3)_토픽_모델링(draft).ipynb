{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW2AfBYeT7ycGbgo91INHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/songmac/2023-Sesac-Lecture-and-Project/blob/master/06_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88(3)_%ED%86%A0%ED%94%BD_%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<자연어처리 기초(3)-토픽 모델링>\n",
        "// 뉴스 이슈 분석, SNS 이슈 트래킹, 산업트렌드 분석, 검색엔진 최적화(SEO)에 활용\n",
        "//주제 추론 : 텍스트 분석 기법 중 하나로서 맥락과 관련된 단서들을 이용하여 의미를 가진 단어들을 클러스터링\n",
        "- 사용자가 직접 토픽 개수를 결정\n",
        "- 분석결과와 사람이 인지하는 것에 차이가 있음\n",
        "\n",
        "\n",
        "<<<LSA(잠재 의미 분석, Latent Semantic Analysis)>>>\n",
        "// 단어들간의 연관관계를 분석함으로써 잠재적인 의미구조를 도출\n",
        "// TDM, DTM, TF-IDF로 중요도를 판단하는 단점을 보완한 모델\n",
        "// '문서는 여러 주제로 구성되어 있고, 각 주제는 단어 집합으로 구성된다'고 가정하며 시작\n",
        "// '동일한 의미를 공유하는 단어들은 같은 문서 안에서 발생한다'는 가정에서 시작\n",
        "-  문서 집합 내 연관성(동시출현 빈도 ex. 배 -> 먹다 or 타다 와 같이 출현)가 높은 단어들을 기준으로 유사한 문서를 추출\n",
        "- 잠재(눈에 보이는 문서외에 다른 파라미터들이 모두 감춰져 있는 상태)의미 분석에서 행렬 분해를 사용\n",
        "<<<<LSA 수행 과정>>>>\n",
        "//decomposition : 행렬분해\n",
        "1) 문서집합을 TDM/DTM/TF-IDF 행렬로 표현\n",
        "2) n개 토픽으로 모델링하겠다고 결정\n",
        "3)  [행렬분해] SVD (특이값 분해, Singular Value Docomposition)\n",
        "- 단어, 주제, 문서 행렬 총 3가지로 분해(좌특이벡터(문서-주제행렬), 특이값(주제 정보량), 우특이벡터(주제-단어행렬))\n",
        "- 좌특이 벡터 : 주제가 무슨 단어로 이루어져 있는가, 각 문서의 주제일 확률이 높은 주제(색이 진한 것)을 각 문서별 할당\n",
        "- 가운데 행렬 : 주제 정보\n",
        "- 우특이 벡터 : 문서가 어떤 주제를 가지고 있는가, 각 주제를 구성할 확률이 높은 단어를 할당\n",
        "- 사용자가 설정한 토픽 수(n)에 따라 결정. 주제 행렬사이즈(차원)을 줄임 -> 데이터 압축, 노이즈 제거\n",
        "4) 유사도 측정 ex. 유클리디언/코사인 유사도 등\n",
        "-  (단어-단어, 문서-문서, 단어-문서) 간 유사도 측정 가능\n",
        "\n",
        "<<<<pLSA(확률적 잠재의미분석, Probabilistic Latent Semantic Analysis)>>>>\n",
        "// 잠재의미분석에서 사용하는 SVD분해 대신, (확률정보 기반)확률적 방법을 사용\n",
        "// '잠재적의미가 존재하고, 이 잠재적의미가 문서와 단어를 연결한다'고 가정\n",
        "// 주제가 먼저 선정되는 것이 LSA와 가장 큰 차이점\n",
        "// 새로운 문서가 들어왔을 때, 이것을 추정하기 어려움\n",
        "// 분석할 문서 수에 따라 파라미터가 선형적으로 증가/연산속도 증가로 많이 사용되지는 않음\n",
        "\n",
        "<<<LDA(잠재 디리클레 할당, Latent Dirichlet Allocation)>>>\n",
        "// 디리클레 다항분포 : 모든 요소를 더한 값이 1인 경우에 대한 확률 값 정의\n",
        "- 문서의 내용을 관찰하여 감춰진 파라미터들을 디리클레 분포를 사용하여 각 단어에 주제를 할당하는 과정\n",
        "- [문서 내 토픽 비율 -> 단어의 토픽 할당] 문서의 내용을 관찰하여 감춰진 파라미터들을 디리클레 다항분포로 주제를 할당한 뒤 그 주제로부터 단어를 추출\n",
        "- 샘플링을 이용하기 때문에 실행시마다 결과가 달라 질 수 있기 때문에 적정한 K를 찾기 어렵고, 알파/베타 값을 잘 튜닝해야 좋은 결과를 얻을 수 있음\n",
        "- 혼잡도(perplexity)가 낮을 수록 응집도(coherence)가 높을 수록 토픽모델링이 잘 되었다고 상대적으로 판단\n",
        "<<<<LDA수행과정>>>>\n",
        "// LDA 추정분포 : 각 문서의 토픽분포 추정, 각 토픽 내 단어분포 추정\n",
        "// 깁스샘플링을 사용하여 각 단어에 토픽을 할당\n",
        "// 깁스 샘플링(Gibbs sampling) : (문서 d에서 단어 w가 토픽 j에 할당될 확률)은 ('토픽 j에 할당된 단어 수')X(문서 d 내 '토픽 j 에 할당된 단어w 수') 에 비례\n",
        "// 토픽 내 토픽등장분포 = 등장빈도 + 알파(0.01)\n",
        "// 토픽 내 단어분포 = 토픽내 단어빈도 + 베타(0.001)\n",
        "// D개의 전체 문서에 k개 토픽이 분포되어있다고 가정\n",
        "1) 토픽 개수 K를 설정 (유니크한 토큰 수 만큼 할당. 사용자가 지정)\n",
        "2) 모든 단어를 k개 토픽 하나에 임의 할당\n",
        "3) 재할당 반복 (일정한 값으로 수렴할 때 까지)"
      ],
      "metadata": {
        "id": "oDsW3P4ohLNm"
      }
    }
  ]
}